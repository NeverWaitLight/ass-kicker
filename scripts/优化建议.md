## 压测结果分析

### 第一轮 固定并发基线测试

**条件**: debug 模式开启 随机 sleep 50-200ms 平均 125ms 固定 50 并发

理论值: 50 并发 / 0.114s 平均延迟 = **438 TPS** 理论上限
实际值: **439 TPS** 几乎完全吻合

说明系统没有额外的严重瓶颈 当前 TPS 完全由 debug sleep 主导

延迟分布:
- P50 117.7ms vs P95 196.1ms 差距不大 分布比较健康
- P99 203.4ms 和 P95 几乎一样 没有严重长尾延迟
- Max 631.9ms 偶发尖刺 可能是 MongoDB 连接抖动或 JVM GC 暂停

### 第二轮 渐进式压测（动态递增负载）

**条件**: 固定模拟发送 100ms 延迟 MongoDB 默认连接池（maxPoolSize=100 minPoolSize=0） 起始并发 1000 每轮递增 25% 持续 30s 当 TPS 低于并发数 x 80% 时停止

  | 轮次 | 并发 | TPS    | 成功率 | P50(ms) | P95(ms) | P99(ms) |
  | ---- | ---- | ------ | ------ | ------- | ------- | ------- |
  | 1    | 1000 | 1518.6 | 100.0% | 594.5   | 704.2   | 4266.0  |
  | 2    | 1897 | 1622.8 | 100.0% | 1114.2  | 1424.5  | 4614.1  |
  | 3    | 2027 | 1718.5 | 100.0% | 1195.8  | 1434.5  | 1596.0  |
  | 4    | 2147 | 1769.1 | 100.0% | 1234.5  | 1491.0  | 1652.5  |
  | 5    | 2211 | 1782.7 | 100.0% | 1270.8  | 1507.2  | 1535.4  |
  | 6    | 2227 | 2030.5 | 100.0% | 1192.8  | 1568.3  | 1856.4  |
  | 7    | 2537 | 2335.3 | 100.0% | 1074.2  | 1792.0  | 2108.0  |
  | 8    | 2918 | 2334.4 | 100.0% | 1242.2  | 1987.8  | 2426.9  |
  | 9    | 2919 | 2160.5 | 100.0% | 1339.1  | 2149.4  | 2532.6  |

停止原因: 第9轮 TPS(2160.5) <= 阈值(2335.2 = 并发2919 x 80%)

**结论**:
- 峰值 TPS **2335** 出现在并发 2537 左右 之后继续加并发 TPS 不再增长反而下降
- 全程 **0 失败** 成功率 100% 系统稳定性良好
- 延迟随并发数增加而上升 P95 从 704ms 涨到 2149ms 说明高并发下排队效应明显
- 当前单机瓶颈约在 **2300 TPS** 附近 受限于 MongoDB 单机写入能力和每请求 3 次 DB 操作

### 每个请求的 I/O 开销

当前一次 send 请求的完整流程:

1. `TemplateManager.fill()` - 模板查询+渲染 有 Caffeine 缓存
2. `ChannelManager.selectChannel()` - 渠道选择 有缓存
3. `sendRecordRepository.save()` - MongoDB 写入 PENDING 记录
4. `channel.send()` - 渠道发送 固定 100ms
5. `sendRecordRepository.findById()` - MongoDB 读取记录
6. `sendRecordRepository.save()` - MongoDB 再次写入更新状态

也就是说 每个请求至少 **3 次 MongoDB 操作** + 1 次 channel send

---

## 优化方向

### 第一 关掉 debug sleep 测真实基线（已完成）

已将模拟发送改为固定 100ms 延迟 在渐进式压测中实测峰值 TPS 达到 **2335** 基线已确认

### 第二 减少 MongoDB 往返次数

当前 `updateRecordStatusAsync` 的逻辑是先 `findById` 再 `save` 两次 MongoDB 操作

```java
SendRecord record = sendRecordRepository.findById(recordId).block();
record.setStatus(status);
sendRecordRepository.save(record).block();
```

可以改成用 MongoDB 的 `updateFirst` 或 `findAndModify` 一次操作完成 省掉一次网络往返

### 第三 批量写入 SendRecord

每个 recipient 都单独 save 一次 PENDING 记录 高并发下 MongoDB 的写入压力很大 可以考虑:

- 用 `saveAll` 批量插入
- 或者引入内存缓冲区 攒一批再刷盘 类似 write-behind 模式
- 甚至考虑是否真的需要先写 PENDING 状态 直接写最终状态能省一半 MongoDB 操作

### 第四 提高并发数（已完成）

渐进式压测已验证 从 1000 并发递增到 2919 并发 TPS 在 2537 并发时达到峰值 **2335** 之后继续加并发 TPS 反降 说明 MongoDB 单机写入已成为瓶颈 而非并发数不够

### 第五 MongoDB 连接池（使用默认配置即可）

实测结论：**数据库连接池无需修改，使用默认配置即可。**

在同等条件下（关闭 debug sleep 等）做了对比压测：
- 默认连接池（maxPoolSize=100、minPoolSize=0）：平均 TPS **2160**
- 显式调大连接池（maxPoolSize=500、minPoolSize=200）：平均 TPS **1669**，不升反降

原因简述：Reactive 驱动下单连接可复用处理多请求，默认 100 已能支撑当前并发；池子过大反而增加客户端与单机 mongod 的内存与调度开销、加剧锁竞争，导致吞吐下降。因此不建议在连接字符串中追加 maxPoolSize/minPoolSize，保持默认即可。

### 第六 reactive 链路中的 block() 调用

你的代码里大量使用了 `.block()` 比如:

```java
renderedContent = templateManager.fill(...).block();
channelEntity = channelManager.selectChannel(...).block();
SendRecord saved = sendRecordRepository.save(record).block();
```

在 WebFlux 的 reactive 体系里调 `.block()` 会阻塞线程 虽然你用了 virtual thread 缓解了这个问题 但如果能改成全链路响应式组合 用 `flatMap`/`map` 串起来 可以进一步减少线程调度开销

### 第七 Kafka consumer 路径优化

你压测的是 `/api/send` 同步路径 直接调用 `processTask` 绕过了 Kafka 如果走 `/api/submit` 的 Kafka 路径 还需要关注:

- 增加 Kafka partition 数量 当前只有 1 个 partition 意味着只有 1 个 consumer 线程
- 增加 consumer 的 `fetch.min.bytes` 和 `max.poll.records` 提高批量消费效率
- Consumer 端的 concurrency 配置

### 第八 减少日志输出

每个请求产生多条 `logExecutionAsync` 日志 高 TPS 下 I/O 型日志写入也会成为瓶颈 可以考虑:

- 把日志级别调高
- 用异步日志 appender 比如 Logback 的 `AsyncAppender`
- 或者采样记录而不是全量记录

---

## 总结一下量级参考

| 场景                                    | TPS 参考         | 状态   |
| --------------------------------------- | ---------------- | ------ |
| debug 随机 sleep 50-200ms 50并发        | ~440             | 已实测 |
| 固定 100ms 延迟 渐进并发至 2537         | **2335（峰值）** | 已实测 |
| 连接池调大 maxPool=500 minPool=200      | 1669（不升反降） | 已实测 |
| 连接池默认 maxPool=100 minPool=0        | 2160             | 已实测 |
| 减少 MongoDB 往返 + 高并发              | 预估 3000-5000   | 待验证 |
| 全链路优化 + 批量写入（连接池保持默认） | 预估 5000-15000+ | 待验证 |

当前单机 MongoDB 默认连接池下峰值 TPS 约 **2335** 瓶颈在 MongoDB 单机写入能力和每请求 3 次 DB 操作 下一步优化方向是减少 MongoDB 往返次数和批量写入